outputs:
  netflow_s3:
    systemFields:
      - cribl_pipe
    streamtags: []
    awsAuthenticationMethod: auto
    signatureVersion: v4
    reuseConnections: true
    rejectUnauthorized: true
    enableAssumeRole: true
    durationSeconds: 3600
    stagePath: $CRIBL_HOME/state/outputs/staging
    addIdToStagePath: true
    destPath: "`streaming/${C.vars['deeptempo_tenant_name']}`"
    objectACL: private
    removeEmptyDirs: true
    partitionExpr: "_floor((_time ? _time : C.Time.now()) / 86400)"
    format: parquet
    baseFileName: "`CriblOut`"
    fileNameSuffix: '`.${C.env["CRIBL_WORKER_ID"]}.${__format}${__compression ===
      "gzip" ? ".gz" : ""}`'
    maxFileSizeMB: 32
    maxOpenFiles: 100
    headerLine: ""
    writeHighWaterMark: 64
    onBackpressure: block
    deadletterEnabled: false
    onDiskFullBackpressure: block
    forceCloseOnShutdown: false
    maxFileOpenTimeSec: 300
    maxFileIdleTimeSec: 30
    maxConcurrentFileParts: 4
    verifyPermissions: true
    maxClosingFilesToBackpressure: 100
    automaticSchema: true
    parquetVersion: PARQUET_2_6
    parquetDataPageVersion: DATA_PAGE_V2
    parquetRowGroupLength: 10000
    parquetPageSize: 1MB
    enableStatistics: true
    enableWritePageIndex: true
    enablePageChecksum: false
    emptyDirCleanupSec: 300
    directoryBatchSize: 1000
    compress: none
    compressionLevel: best_speed
    disabled: false
    type: s3
    bucket: "'tempo-landing'"
    region: us-west-2
    description: NetFlow data routed to S3 for Spark processing
    __template_region: deeptempo_s3_region
    __template_bucket: deeptempo_s3_bucket
